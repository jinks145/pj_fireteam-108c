{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import matrix as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "I used pytorch because it has the same functionality while easily scalable and the capacity to run Nvidia GPUs(Thanks Daddy Jensen LOL).  \n",
    "But this could be easily adopted into numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is PQ Factorization, an implementation of the matrix factorization algorithm using PyTorch. nn Module is used to define the model and the forward pass. `nn.Module` in pytorch is usually used to define a neural network model, but it can be used to define classical models as well. Usually, the main components of the model are forward pass, backward pass, and optimization.  \n",
    "The forward pass is the computation of the output of the model given the input, and is implemented in the forward method of the nn.Module class. The backward pass is the computation of the gradients of the model parameters with respect to the loss, and is implemented in the backward method of the nn.Module class.\n",
    "The optimization is the process of updating the model parameters using the gradients computed in the backward pass, and usually implemented using an optimizer such as SGD or Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c226d50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PQFactorization(nn.Module):\n",
    "\tdef __init__(self, target, latent_factors=2, processor=\"cpu\"):\n",
    "\t\tsuper(PQFactorization, self).__init__()\n",
    "\t\tself.R = torch.tensor(target)\n",
    "\t\tself.latent_factors = latent_factors\n",
    "\t\tself.device = processor\n",
    "\t\t\n",
    "\tdef forward(self, P, Q, processor=\"cpu\"):\n",
    "\t\tif type(P) == np.ndarray:\n",
    "\t\t\tP = torch.tensor(P, device=processor)\n",
    "\t\tif type(Q) == np.ndarray:\n",
    "\t\t\tQ = torch.tensor(Q, device=processor)\n",
    "\t\t\t\n",
    "\t\treturn torch.matmul(P, Q.T)\n",
    "\t\n",
    "\n",
    "\n",
    "\tdef FrobeniousLoss(self,E,P, Q, beta):\n",
    "\t\treturn 1/2* torch.norm(E - R, p='fro') + beta / 2 * torch.norm(P, p='fro') + torch.norm(Q, p='fro')\n",
    "\t\n",
    "\tdef fit(self, epochs=1000, alpha=0.01, beta=0.02, patience=10, min_delta=0.001,  processor=\"cpu\"):\n",
    "\t\tPik = torch.randn(self.R.shape[0], self.latent_factors, device=self.device)\n",
    "\t\tQkj = torch.randn(self.R.shape[1], self.latent_factors, device=self.device)\n",
    "\t\tbest_loss = float('inf')\n",
    "\t\tcounter = 0\n",
    "\n",
    "\t\tfor epoch in range(epochs):\n",
    "\t\t\terror_matrix = self.forward(Pik, Qkj) - self.R\n",
    "\t\t\tloss = self.FrobeniousLoss(error_matrix, Pik, Qkj, beta)\n",
    "\t\t\tif loss < best_loss - min_delta:\n",
    "\t\t\t\tbest_loss = loss\n",
    "\t\t\t\tcounter = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tcounter += 1\n",
    "\n",
    "\t\t\tif counter >= patience:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch+1} with loss {loss.item():.4f}\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tPik -= alpha * (2 * error_matrix.matmul(Qkj) - beta * Pik)\n",
    "\t\t\tQkj -= alpha * (2 * error_matrix.T.matmul(Pik) - beta * Qkj)\n",
    "\t\t\tif (epoch + 1) % 100 == 0:  # Print loss every 100 epochs\n",
    "\t\t\t\tprint(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\t\treturn Pik.matmul(Qkj.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([\n",
    "     [4.5,2.3,0,1.2],\n",
    "     [4.6,0,0,1.25],\n",
    "     [1.0,1.2,0,5.6],\n",
    "     [1.8,0,0,4.3],\n",
    "     [0,1.4,5.32,4.3],\n",
    "    ], dtype=np.float32)\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 92 with loss 10.6888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7594,  1.2884,  0.2733,  1.2455],\n",
       "        [ 4.3465,  1.0454, -0.2805,  1.2030],\n",
       "        [ 1.1452,  0.6191,  0.1547,  5.6327],\n",
       "        [ 1.6536,  0.5889, -0.1621,  4.2799],\n",
       "        [-0.0410,  1.5644,  5.2852,  4.2956]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PQFactorization(R, latent_factors=K)\n",
    "model.fit(epochs=10000, alpha=0.01, beta=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_parquet('matrix.parquet.gzip')\n",
    "display(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = matrix.to_numpy()\n",
    "R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
