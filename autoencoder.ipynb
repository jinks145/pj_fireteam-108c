{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Interaction Matrix (first 10 users):\n",
      "[[0 0 2 0 0 0 0 2 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 5 0\n",
      "  0 0 0 0 0 0 4 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 4]\n",
      " [4 0 0 0 0 3 0 0 0 0 2 0 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 3 4 0 0]\n",
      " [0 4 2 0 3 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 2 5 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 1 0 0 0 1 2 0 4 0 0 0 0 0 0 0 5 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  4 0 0 0 0 0 0 0 0 0 0 0 0 3]\n",
      " [0 0 3 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 3 4 4 0 0 0\n",
      "  0 0 0 3 0 0 0 0 0 0 0 0 5 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 2 0 0 0 0 0 5 0 0 0 2\n",
      "  0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 4 0 0 0 0 0 0 3 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 2 3 0 0 4 0 0\n",
      "  0 0 0 5 0 0 0 0 0 0 0 0 1 5]\n",
      " [0 0 0 0 0 0 0 1 4 5 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 5 0 0\n",
      "  0 3 0 0 0 5 5 0 0 0 0 1 0 0]]\n",
      "\n",
      "Noisy Interaction Matrix (first 10 users):\n",
      "[[5.78074634e-01 4.46185231e-01 2.27021527e+00 0.00000000e+00\n",
      "  2.03525528e-01 0.00000000e+00 0.00000000e+00 1.51859987e+00\n",
      "  0.00000000e+00 4.49461699e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.28733629e-01\n",
      "  4.92695093e-01 0.00000000e+00 0.00000000e+00 1.31876782e-01\n",
      "  0.00000000e+00 3.23495299e-01 2.40240172e-01 5.04186213e-01\n",
      "  3.83737326e-01 3.88926864e-01 3.18314004e+00 4.00421351e-01\n",
      "  0.00000000e+00 1.25278477e-02 0.00000000e+00 2.57957578e-01\n",
      "  0.00000000e+00 0.00000000e+00 4.93299007e+00 5.15208483e-01\n",
      "  9.56640989e-02 0.00000000e+00 9.17162821e-02 0.00000000e+00\n",
      "  0.00000000e+00 2.98690856e-01 3.73606420e+00 0.00000000e+00\n",
      "  3.61775470e+00 6.36835575e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.34071631e-02 1.57742634e-01 0.00000000e+00 3.57410729e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.08011011e-02\n",
      "  0.00000000e+00 2.02684477e-01 2.97065783e+00 5.53378224e-01\n",
      "  0.00000000e+00 4.15064812e-01 4.33540165e-01 2.56923765e-01\n",
      "  1.66542268e+00 1.56949669e-01 1.03994004e-01 0.00000000e+00\n",
      "  0.00000000e+00 3.83398682e-01 9.48342979e-01 1.57136545e-01\n",
      "  1.69865470e-02 1.27888843e-01 1.72501519e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.26044345e-03 0.00000000e+00\n",
      "  2.59779620e+00 0.00000000e+00 1.60856441e-01 1.57386795e-01\n",
      "  3.42360497e-01 1.54930791e-02 2.23185599e-01 0.00000000e+00\n",
      "  0.00000000e+00 1.81169659e-01 0.00000000e+00 0.00000000e+00\n",
      "  4.00410920e-01 1.45061791e-01 0.00000000e+00 1.46400318e-01\n",
      "  2.35376060e-01 4.00859404e+00]\n",
      " [4.19222641e+00 1.74974233e-01 3.20078015e-01 0.00000000e+00\n",
      "  0.00000000e+00 3.22582769e+00 1.21427342e-01 5.35398014e-02\n",
      "  7.94728547e-02 3.81950527e-01 1.99960673e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.96929932e+00 8.20254087e-01 1.14311695e+00\n",
      "  2.17853174e-01 2.73455605e-02 0.00000000e+00 1.58374950e-01\n",
      "  0.00000000e+00 7.22509027e-02 3.97606120e-02 2.29272202e-01\n",
      "  3.28502923e-01 1.01967290e-01 2.15990260e-01 1.23422287e-01\n",
      "  5.79348207e-01 3.03559154e-01 0.00000000e+00 0.00000000e+00\n",
      "  4.95918941e+00 4.90622908e-01 1.96422234e-01 1.72801375e-01\n",
      "  1.34245241e+00 5.56937326e-03 0.00000000e+00 2.77630478e-01\n",
      "  0.00000000e+00 3.09926212e-01 0.00000000e+00 1.91044107e-01\n",
      "  0.00000000e+00 2.87537336e-01 3.48576021e+00 4.43518305e+00\n",
      "  8.08444619e-02 0.00000000e+00]\n",
      " [0.00000000e+00 4.03128958e+00 2.10462546e+00 2.90278256e-01\n",
      "  2.86029339e+00 4.81439203e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.43701085e-01 0.00000000e+00 6.85729608e-02\n",
      "  4.00745773e+00 0.00000000e+00 8.60498473e-02 0.00000000e+00\n",
      "  5.24460785e-02 0.00000000e+00 0.00000000e+00 4.05869097e-01\n",
      "  3.86648327e-01 1.56886429e-02 0.00000000e+00 2.27011830e-01\n",
      "  2.32655853e-01 6.07960701e-01 3.01074529e+00 3.61766182e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.65920734e+00 4.84322071e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.83553728e-02 3.06874007e-01 0.00000000e+00\n",
      "  2.11281836e-01 2.12962806e-01 5.32316566e-01 0.00000000e+00\n",
      "  2.88734972e-01 0.00000000e+00 0.00000000e+00 1.07417129e-01\n",
      "  1.43630370e-01 4.06110018e-01]\n",
      " [1.57818630e-01 3.63361144e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.55484034e-02 1.32060754e+00 3.91960353e-01 1.37950361e-01\n",
      "  0.00000000e+00 6.93628192e-01 1.85152292e+00 0.00000000e+00\n",
      "  4.04629469e+00 1.32230133e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.24145913e-01 0.00000000e+00 4.52413648e-01\n",
      "  5.00000000e+00 3.84990740e+00 3.10500979e-01 5.06894171e-01\n",
      "  0.00000000e+00 5.00037730e-01 4.61760573e-02 0.00000000e+00\n",
      "  0.00000000e+00 2.50704121e-02 1.19971611e-01 5.96762180e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.00528669e+00 2.34677959e-02 5.79474904e-02 1.22902028e-01\n",
      "  0.00000000e+00 8.28572661e-02 0.00000000e+00 1.38774708e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 4.39157099e-01\n",
      "  3.76630306e-01 2.78551316e+00]\n",
      " [2.56175816e-01 1.53897360e-01 3.16191936e+00 1.69651508e-01\n",
      "  1.51737526e-01 6.67361021e-02 0.00000000e+00 1.69067711e-01\n",
      "  2.54784751e+00 0.00000000e+00 0.00000000e+00 1.29682288e-01\n",
      "  0.00000000e+00 2.34635517e-01 0.00000000e+00 0.00000000e+00\n",
      "  2.14559853e-01 1.17417676e-02 3.91758054e-01 1.07397783e+00\n",
      "  0.00000000e+00 5.36881294e-03 0.00000000e+00 1.87740833e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 7.75901377e-01\n",
      "  5.12792647e-01 1.73768084e-02 3.35789418e+00 4.58118820e+00\n",
      "  4.21861410e+00 2.94268161e-01 1.24377683e-01 3.46968949e-01\n",
      "  8.07163715e-02 0.00000000e+00 2.91988194e-01 2.69547629e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 6.06223106e-01 7.61757046e-02\n",
      "  5.00000000e+00 2.13670909e-01]\n",
      " [0.00000000e+00 3.04922611e-02 4.02991235e-01 2.13980883e-01\n",
      "  1.21140890e-01 0.00000000e+00 2.50118762e-01 0.00000000e+00\n",
      "  1.36090279e-01 3.73827606e-01 0.00000000e+00 0.00000000e+00\n",
      "  5.39659522e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.98341513e+00 3.61455202e-01 0.00000000e+00\n",
      "  1.30031407e-01 0.00000000e+00 3.16610783e-01 0.00000000e+00\n",
      "  1.39545217e-01 2.11141753e+00 0.00000000e+00 2.38648336e-02\n",
      "  1.13453545e-01 2.11534247e-01 0.00000000e+00 4.74695587e+00\n",
      "  1.30543068e-01 7.97661617e-02 0.00000000e+00 2.02480674e+00\n",
      "  2.65614241e-01 5.47331981e-02 2.35914305e-01 0.00000000e+00\n",
      "  1.69999599e-01 0.00000000e+00 0.00000000e+00 1.50287999e-02\n",
      "  3.18252254e+00 4.89260465e-01 0.00000000e+00 3.25323761e-01\n",
      "  2.84329712e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.12569600e-01\n",
      "  2.63729077e-02 0.00000000e+00 0.00000000e+00 4.74668646e+00\n",
      "  0.00000000e+00 5.96688390e-01 5.70093572e-01 5.08524179e-01\n",
      "  8.42685904e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.56604338e+00 0.00000000e+00 4.98899482e-02\n",
      "  6.43250465e-01 5.11370182e-01 1.03770383e-01 1.92742541e-01\n",
      "  9.38813746e-01 2.05610216e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.34545743e-01 0.00000000e+00 2.56218970e-01\n",
      "  0.00000000e+00 0.00000000e+00 1.99912146e-01 0.00000000e+00\n",
      "  9.37118530e-01 4.98964004e-02 4.41091150e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  9.00604308e-01 2.53073990e-01 2.96234220e-01 0.00000000e+00\n",
      "  0.00000000e+00 2.47309268e-01]\n",
      " [7.40997773e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.28870964e+00 0.00000000e+00 4.58141379e-02 0.00000000e+00\n",
      "  2.83381790e-01 0.00000000e+00 2.96138734e-01 3.34370232e+00\n",
      "  9.57456529e-01 9.17097569e-01 0.00000000e+00 2.33034298e-01\n",
      "  2.05162585e-01 6.02623165e-01 0.00000000e+00 1.80055290e-01\n",
      "  0.00000000e+00 0.00000000e+00 9.86142397e-01 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 9.42843258e-02 4.24665958e-02\n",
      "  3.10445309e-01 1.81206870e+00 2.84547234e+00 2.07086995e-01\n",
      "  0.00000000e+00 4.34098387e+00 0.00000000e+00 4.25993860e-01\n",
      "  2.54555702e-01 0.00000000e+00 2.00568065e-01 5.00000000e+00\n",
      "  2.06970528e-01 0.00000000e+00 1.13410950e-02 0.00000000e+00\n",
      "  0.00000000e+00 3.56842220e-01 2.28208348e-01 0.00000000e+00\n",
      "  5.84834576e-01 5.00000000e+00]\n",
      " [0.00000000e+00 9.88464616e-03 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.47976631e-01 4.77418691e-01 9.63756025e-01\n",
      "  3.85509372e+00 5.00000000e+00 2.31452603e-02 0.00000000e+00\n",
      "  0.00000000e+00 3.25815976e-01 3.28992367e-01 0.00000000e+00\n",
      "  1.98130086e-02 9.99767840e-01 4.86181341e-02 3.58787447e-01\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.17190015e-02\n",
      "  1.24517448e-01 0.00000000e+00 8.50190401e-01 5.86042225e-01\n",
      "  6.14604473e-01 2.67358851e+00 4.86508399e-01 2.55379707e-01\n",
      "  0.00000000e+00 4.81735182e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.68913698e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.65723419e+00 4.86537457e+00 0.00000000e+00\n",
      "  1.14909351e-01 0.00000000e+00 3.53982627e-01 9.00571644e-01\n",
      "  1.94851279e-01 2.84875557e-02]]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a user-item interaction matrix (100 users, 50 items) with sparsity\n",
    "num_users = 100\n",
    "num_items = 50\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_items))  # Random interactions from 0 to 5\n",
    "\n",
    "# Introduce sparsity by setting a high percentage of interactions to 0\n",
    "sparsity = 0.8  # 80% of the interactions will be set to 0\n",
    "mask = np.random.rand(*interaction_matrix.shape) < sparsity\n",
    "interaction_matrix[mask] = 0\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "interaction_tensor = torch.tensor(interaction_matrix, dtype=torch.float32)\n",
    "\n",
    "# Add noise to the input data\n",
    "def add_noise(data, noise_factor=0.3):\n",
    "    noisy_data = data + noise_factor * torch.randn_like(data)\n",
    "    noisy_data = torch.clamp(noisy_data, 0., 5.)  # Ensure values stay within the interaction range\n",
    "    return noisy_data\n",
    "\n",
    "noisy_interaction_tensor = add_noise(interaction_tensor)\n",
    "\n",
    "# Print the original and noisy matrices (first 10 users for brevity)\n",
    "print(\"Original Interaction Matrix (first 10 users):\")\n",
    "print(interaction_matrix[:10])  # Print only the first 10 users for brevity\n",
    "print(\"\\nNoisy Interaction Matrix (first 10 users):\")\n",
    "print(noisy_interaction_tensor[:10].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\tdef __init__(self, input_dim, bottleneck_size, device='cpu'):\n",
    "\t\tsuper(AutoEncoder, self).__init__()\n",
    "\t\tself.device = device\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, bottleneck_size)\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(bottleneck_size, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, input_dim)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef fit(self, batches, n_epochs=100, min_delta=0.0001, lr=0.001, patience=10):\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\tbest_loss = float('inf')\n",
    "\t\tpatience_counter = 0\n",
    "\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\tepoch_loss = 0.0\n",
    "\t\t\tfor batch in batches:\n",
    "\t\t\t\tbatch = batch[0].to(self.device)  # Move batch to device\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\toutput = self.forward(batch)\n",
    "\t\t\t\tloss = criterion(output, batch)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\t\tepoch_loss /= len(batches)\n",
    "\n",
    "\t\t\tif epoch_loss < best_loss - min_delta:\n",
    "\t\t\t\tbest_loss = epoch_loss\n",
    "\t\t\t\tpatience_counter = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tpatience_counter += 1\n",
    "\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\t\treturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "train = TensorDataset(noisy_interaction_tensor)\n",
    "batches = DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.7445\n",
      "Epoch [2/1000], Loss: 1.7712\n",
      "Epoch [3/1000], Loss: 1.6870\n",
      "Epoch [4/1000], Loss: 1.7873\n",
      "Epoch [5/1000], Loss: 1.7704\n",
      "Epoch [6/1000], Loss: 1.7228\n",
      "Epoch [7/1000], Loss: 1.5618\n",
      "Epoch [8/1000], Loss: 1.7257\n",
      "Epoch [9/1000], Loss: 1.7183\n",
      "Epoch [10/1000], Loss: 1.5668\n",
      "Epoch [11/1000], Loss: 1.6816\n",
      "Epoch [12/1000], Loss: 1.7005\n",
      "Epoch [13/1000], Loss: 1.6839\n",
      "Epoch [14/1000], Loss: 1.6275\n",
      "Epoch [15/1000], Loss: 1.6543\n",
      "Epoch [16/1000], Loss: 1.6534\n",
      "Early stopping at epoch 17 with loss 1.8313\n"
     ]
    }
   ],
   "source": [
    "test = AutoEncoder(50, 10)\n",
    "test.fit(batches, 1000, 0.0001, 0.0001, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('matrix.csv.gzip', compression='gzip', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869924201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings.notna().count().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = user_ratings.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = torch.tensor(user_ratings.to_numpy(), dtype=torch.float32, device=device)\n",
    "batch_size = 32 \n",
    "train = TensorDataset(tensors)\n",
    "batches = DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.0056\n",
      "Epoch [2/1000], Loss: 0.0054\n",
      "Epoch [3/1000], Loss: 0.0053\n",
      "Epoch [4/1000], Loss: 0.0052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoEncoder(tensors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m, in \u001b[0;36mAutoEncoder.fit\u001b[0;34m(self, batches, n_epochs, min_delta, lr, patience)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     32\u001b[0m \tepoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move batch to device\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(tensors.shape[-1], 10)\n",
    "model.fit(batches, 1000, 0.0001, 0.0001, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
