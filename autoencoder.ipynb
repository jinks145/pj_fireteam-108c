{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# :Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the autoencoder, we chose Pytorch for its similarity to numpy and compatibility to gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Matrix below was used to debug and test the performance before applying it to the actual matrix.\n",
    "The matrix is a sparse integer matrix of 100 users by 50 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a user-item interaction matrix (100 users, 50 items) with sparsity\n",
    "num_users = 10000\n",
    "num_items = 500\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_items))  # Random interactions from 0 to 5\n",
    "\n",
    "# Introduce sparsity by setting a high percentage of interactions to 0\n",
    "sparsity = 0.8  # 80% of the interactions will be set to 0\n",
    "mask = np.random.rand(*interaction_matrix.shape) < sparsity\n",
    "interaction_matrix[mask] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, coo_matrix):\n",
    "        self.data = torch.tensor(coo_matrix.data, dtype=torch.float32)\n",
    "        self.row = torch.tensor(coo_matrix.row, dtype=torch.long)\n",
    "        self.col = torch.tensor(coo_matrix.col, dtype=torch.long)\n",
    "        self.shape = coo_matrix.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.row[idx], self.col[idx], self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_coo(df):\n",
    "    user_mapping = {user_id: idx for idx, user_id in enumerate(df['User_id'].unique())}\n",
    "    book_mapping = {book_id: idx for idx, book_id in enumerate(df['book_id'].unique())}\n",
    "    \n",
    "    # Map the User_id and book_id columns to their integer indices\n",
    "    df['user_idx'] = df['User_id'].map(user_mapping)\n",
    "    df['book_idx'] = df['book_id'].map(book_mapping)\n",
    "    \n",
    "    # Create the COO matrix\n",
    "    row = df['user_idx'].values\n",
    "    col = df['book_idx'].values\n",
    "    data = df['review/score'].values\n",
    "    num_users = len(user_mapping)\n",
    "    num_books = len(book_mapping)\n",
    "    \n",
    "    coo = coo_matrix((data, (row, col)), shape=(num_users, num_books))\n",
    "    return coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\tdef __init__(self, input_dim, bottleneck_size, device='cpu'):\n",
    "\t\tsuper(AutoEncoder, self).__init__()\n",
    "\t\tself.device = device\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, bottleneck_size)\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(bottleneck_size, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, input_dim)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "    \n",
    "\tdef fit(self, batches, input_dim, n_epochs=100, min_delta=0.0001, lr=0.001, patience=10):\n",
    "\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\tbest_loss = float('inf')\n",
    "\t\tpatience_counter = 0\n",
    "\t\tepoch_losses = []\n",
    "\t\tepochs = []\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\tepoch_loss = 0.0\n",
    "\t\t\tfor row, col, value in batches:\n",
    "\t\t\t\trow, col, value = row.to(self.device), col.to(self.device), value.to(self.device)\n",
    "                \n",
    "                # Create an input vector with the same length as the number of unique columns\n",
    "\t\t\t\tinput_vector = torch.zeros((len(row), input_dim), device=self.device)\n",
    "\t\t\t\tinput_vector[torch.arange(len(row)), col] = value\n",
    "                \n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\toutput = self.forward(input_vector)\n",
    "                \n",
    "                # Compute the loss only on the known values\n",
    "\t\t\t\tloss = criterion(output[torch.arange(len(row)), col], value)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "        \n",
    "\t\t\tepoch_loss /= len(batches)\n",
    "\n",
    "\t\t\tif epoch_loss < best_loss - min_delta:\n",
    "\t\t\t\tbest_loss = epoch_loss\n",
    "\t\t\t\tpatience_counter = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tpatience_counter += 1\n",
    "\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tepoch_losses.append(epoch_loss)\n",
    "\t\t\tepochs.append(epoch+1)\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\t\treturn epochs, epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_by_bottleneck(user_ratings: pd.DataFrame, device: torch.device, bottleneck: int = 10):\n",
    "\n",
    "    # Create the COO matrix\n",
    "    coo = convert_df_coo(user_ratings)\n",
    "    dataset = SparseDataset(coo)\n",
    "    model = AutoEncoder(coo.shape[1], 10, device=device).to(device)\n",
    "    batch_size = 128\n",
    "    \n",
    "    batches = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    epochs, losses = model.fit(batches, coo.shape[1], n_epochs=200, lr=0.001, patience=20)\n",
    "    \n",
    "    pd.DataFrame({\"epochs\": epochs, \"training losses\": losses}).to_csv(f'sample_model/{bottleneck}.csv')\n",
    "    torch.save(model, f'models/model/model_k={bottleneck}.pt')\n",
    "    torch.save(model.state_dict(), f'models/weights/smodel_k_weights_{bottleneck}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coo = coo_matrix(interaction_matrix, (num_users, num_items))\n",
    "# dense_matrix = torch.tensor(coo.data(), dtype=torch.float32).to(device)\n",
    "# dataset = TensorDataset(dense_matrix)\n",
    "# bottleneck_size = 32\n",
    "# batch_size = 128\n",
    "# train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(num_items, bottleneck_size, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# epochs, epoch_losses = model.fit(train_loader, min_delta=0.001, n_epochs=1000, lr=0.001, patience=10)\n",
    "\n",
    "# # Plot the training loss over epochs\n",
    "# plt.plot(epochs, epoch_losses, label='Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then trained the model using LambdaLab's nvidia A10 gpu VM instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `testing_mode` is a checkpoint variable to signify that we are testing the model on the local end. If set to false, it will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('ratings_user_tagged.csv.gzip', compression='gzip')\n",
    "# user_ratings = user_ratings.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_id</th>\n",
       "      <th>review/score</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0826414346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0826414346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>A2F6NONFUDB6UK</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0826414346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>A14OJS0VWMOSWO</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0826414346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A373VVEU6Z9M0N</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0829814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406027</th>\n",
       "      <td>34950</td>\n",
       "      <td>A2PK3NTC9RMEF4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0786182431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406028</th>\n",
       "      <td>34958</td>\n",
       "      <td>A32ZKBXJJ45BRY</td>\n",
       "      <td>3.0</td>\n",
       "      <td>B00085PL4C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406029</th>\n",
       "      <td>34967</td>\n",
       "      <td>A25JH6CO4DVINS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0255364520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406030</th>\n",
       "      <td>34969</td>\n",
       "      <td>AOFGOUMXLMVZS</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B000NSLVCU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406031</th>\n",
       "      <td>34970</td>\n",
       "      <td>A1SMUB9ASL5L9Y</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B000NSLVCU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406032 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         User_id  review/score     book_id\n",
       "0                1  A30TK6U7DNS82R           5.0  0826414346\n",
       "1                3  A2MVUWT453QH61           4.0  0826414346\n",
       "2                5  A2F6NONFUDB6UK           4.0  0826414346\n",
       "3                6  A14OJS0VWMOSWO           5.0  0826414346\n",
       "4               11  A373VVEU6Z9M0N           5.0  0829814000\n",
       "...            ...             ...           ...         ...\n",
       "406027       34950  A2PK3NTC9RMEF4           3.0  0786182431\n",
       "406028       34958  A32ZKBXJJ45BRY           3.0  B00085PL4C\n",
       "406029       34967  A25JH6CO4DVINS           4.0  0255364520\n",
       "406030       34969   AOFGOUMXLMVZS           4.0  B000NSLVCU\n",
       "406031       34970  A1SMUB9ASL5L9Y           4.0  B000NSLVCU\n",
       "\n",
       "[406032 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6842"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings['User_id'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training on colab, we utilized the pytorch imports the weights and the model architecture into the \n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is a significant difference between bottlenecks, we decided to run it with different bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottlenecks = [16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we used the batch size of 128 and parameters of 1000 epoch, learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== training k=16 model ===\n",
      "\n",
      "Epoch [1/200], Loss: 6.0190\n",
      "Epoch [2/200], Loss: 1.5349\n",
      "Epoch [3/200], Loss: 0.8493\n",
      "Epoch [4/200], Loss: 0.7627\n",
      "Epoch [5/200], Loss: 0.6280\n",
      "Epoch [6/200], Loss: 0.5494\n",
      "Epoch [7/200], Loss: 0.4556\n",
      "Epoch [8/200], Loss: 0.4256\n",
      "Epoch [9/200], Loss: 0.3695\n",
      "Epoch [10/200], Loss: 0.3435\n",
      "Epoch [11/200], Loss: 0.3082\n",
      "Epoch [12/200], Loss: 0.2886\n",
      "Epoch [13/200], Loss: 0.2609\n",
      "Epoch [14/200], Loss: 0.2482\n",
      "Epoch [15/200], Loss: 0.2282\n",
      "Epoch [16/200], Loss: 0.2179\n",
      "Epoch [17/200], Loss: 0.2035\n",
      "Epoch [18/200], Loss: 0.1932\n",
      "Epoch [19/200], Loss: 0.1825\n",
      "Epoch [20/200], Loss: 0.1747\n",
      "Epoch [21/200], Loss: 0.1652\n",
      "Epoch [22/200], Loss: 0.1567\n",
      "Epoch [23/200], Loss: 0.1517\n",
      "Epoch [24/200], Loss: 0.1420\n",
      "Epoch [25/200], Loss: 0.1378\n",
      "Epoch [26/200], Loss: 0.1297\n",
      "Epoch [27/200], Loss: 0.1267\n",
      "Epoch [28/200], Loss: 0.1194\n",
      "Epoch [29/200], Loss: 0.1164\n",
      "Epoch [30/200], Loss: 0.1111\n",
      "Epoch [31/200], Loss: 0.1062\n",
      "Epoch [32/200], Loss: 0.1033\n",
      "Epoch [33/200], Loss: 0.0988\n",
      "Epoch [34/200], Loss: 0.0958\n",
      "Epoch [35/200], Loss: 0.0919\n",
      "Epoch [36/200], Loss: 0.0886\n",
      "Epoch [37/200], Loss: 0.0863\n",
      "Epoch [38/200], Loss: 0.0832\n",
      "Epoch [39/200], Loss: 0.0810\n",
      "Epoch [40/200], Loss: 0.0789\n",
      "Epoch [41/200], Loss: 0.0764\n",
      "Epoch [42/200], Loss: 0.0740\n",
      "Epoch [43/200], Loss: 0.0733\n",
      "Epoch [44/200], Loss: 0.0699\n",
      "Epoch [45/200], Loss: 0.0695\n",
      "Epoch [46/200], Loss: 0.0669\n",
      "Epoch [47/200], Loss: 0.0666\n",
      "Epoch [48/200], Loss: 0.0640\n",
      "Epoch [49/200], Loss: 0.0634\n",
      "Epoch [50/200], Loss: 0.0617\n",
      "Epoch [51/200], Loss: 0.0610\n",
      "Epoch [52/200], Loss: 0.0596\n",
      "Epoch [53/200], Loss: 0.0586\n",
      "Epoch [54/200], Loss: 0.0574\n",
      "Epoch [55/200], Loss: 0.0567\n",
      "Epoch [56/200], Loss: 0.0552\n",
      "Epoch [57/200], Loss: 0.0549\n",
      "Epoch [58/200], Loss: 0.0529\n",
      "Epoch [59/200], Loss: 0.0527\n",
      "Epoch [60/200], Loss: 0.0512\n",
      "Epoch [61/200], Loss: 0.0510\n",
      "Epoch [62/200], Loss: 0.0500\n",
      "Epoch [63/200], Loss: 0.0495\n",
      "Epoch [64/200], Loss: 0.0489\n",
      "Epoch [65/200], Loss: 0.0474\n",
      "Epoch [66/200], Loss: 0.0477\n",
      "Epoch [67/200], Loss: 0.0466\n",
      "Epoch [68/200], Loss: 0.0458\n",
      "Epoch [69/200], Loss: 0.0454\n",
      "Epoch [70/200], Loss: 0.0454\n",
      "Epoch [71/200], Loss: 0.0440\n",
      "Epoch [72/200], Loss: 0.0431\n",
      "Epoch [73/200], Loss: 0.0434\n",
      "Epoch [74/200], Loss: 0.0420\n",
      "Epoch [75/200], Loss: 0.0424\n",
      "Epoch [76/200], Loss: 0.0414\n",
      "Epoch [77/200], Loss: 0.0408\n",
      "Epoch [78/200], Loss: 0.0402\n",
      "Epoch [79/200], Loss: 0.0400\n",
      "Epoch [80/200], Loss: 0.0394\n",
      "Epoch [81/200], Loss: 0.0389\n",
      "Epoch [82/200], Loss: 0.0384\n",
      "Epoch [83/200], Loss: 0.0387\n",
      "Epoch [84/200], Loss: 0.0382\n",
      "Epoch [85/200], Loss: 0.0373\n",
      "Epoch [86/200], Loss: 0.0373\n",
      "Epoch [87/200], Loss: 0.0366\n",
      "Epoch [88/200], Loss: 0.0365\n",
      "Epoch [89/200], Loss: 0.0359\n",
      "Epoch [90/200], Loss: 0.0356\n",
      "Epoch [91/200], Loss: 0.0352\n",
      "Epoch [92/200], Loss: 0.0349\n",
      "Epoch [93/200], Loss: 0.0350\n",
      "Epoch [94/200], Loss: 0.0344\n",
      "Epoch [95/200], Loss: 0.0339\n",
      "Epoch [96/200], Loss: 0.0342\n",
      "Epoch [97/200], Loss: 0.0339\n",
      "Epoch [98/200], Loss: 0.0332\n",
      "Epoch [99/200], Loss: 0.0332\n",
      "Epoch [100/200], Loss: 0.0329\n",
      "Epoch [101/200], Loss: 0.0331\n",
      "Epoch [102/200], Loss: 0.0325\n",
      "Epoch [103/200], Loss: 0.0321\n",
      "Epoch [104/200], Loss: 0.0320\n",
      "Epoch [105/200], Loss: 0.0316\n",
      "Epoch [106/200], Loss: 0.0307\n",
      "Epoch [107/200], Loss: 0.0315\n",
      "Epoch [108/200], Loss: 0.0312\n",
      "Epoch [109/200], Loss: 0.0305\n",
      "Epoch [110/200], Loss: 0.0306\n",
      "Epoch [111/200], Loss: 0.0301\n",
      "Epoch [112/200], Loss: 0.0302\n",
      "Epoch [113/200], Loss: 0.0298\n",
      "Epoch [114/200], Loss: 0.0305\n",
      "Epoch [115/200], Loss: 0.0296\n",
      "Epoch [116/200], Loss: 0.0298\n",
      "Epoch [117/200], Loss: 0.0294\n",
      "Epoch [118/200], Loss: 0.0288\n",
      "Epoch [119/200], Loss: 0.0291\n",
      "Epoch [120/200], Loss: 0.0287\n",
      "Epoch [121/200], Loss: 0.0285\n",
      "Epoch [122/200], Loss: 0.0283\n",
      "Epoch [123/200], Loss: 0.0282\n",
      "Epoch [124/200], Loss: 0.0282\n",
      "Epoch [125/200], Loss: 0.0277\n",
      "Epoch [126/200], Loss: 0.0276\n",
      "Epoch [127/200], Loss: 0.0276\n",
      "Epoch [128/200], Loss: 0.0274\n",
      "Epoch [129/200], Loss: 0.0273\n",
      "Epoch [130/200], Loss: 0.0275\n",
      "Epoch [131/200], Loss: 0.0270\n",
      "Epoch [132/200], Loss: 0.0272\n",
      "Epoch [133/200], Loss: 0.0264\n",
      "Epoch [134/200], Loss: 0.0266\n",
      "Epoch [135/200], Loss: 0.0266\n",
      "Epoch [136/200], Loss: 0.0263\n",
      "Epoch [137/200], Loss: 0.0265\n",
      "Epoch [138/200], Loss: 0.0259\n",
      "Epoch [139/200], Loss: 0.0262\n",
      "Epoch [140/200], Loss: 0.0256\n",
      "Epoch [141/200], Loss: 0.0259\n",
      "Epoch [142/200], Loss: 0.0253\n",
      "Epoch [143/200], Loss: 0.0254\n",
      "Epoch [144/200], Loss: 0.0255\n",
      "Epoch [145/200], Loss: 0.0253\n",
      "Epoch [146/200], Loss: 0.0249\n",
      "Epoch [147/200], Loss: 0.0249\n",
      "Epoch [148/200], Loss: 0.0251\n",
      "Epoch [149/200], Loss: 0.0247\n",
      "Epoch [150/200], Loss: 0.0248\n",
      "Epoch [151/200], Loss: 0.0243\n",
      "Epoch [152/200], Loss: 0.0245\n",
      "Epoch [153/200], Loss: 0.0244\n",
      "Epoch [154/200], Loss: 0.0243\n",
      "Epoch [155/200], Loss: 0.0240\n",
      "Epoch [156/200], Loss: 0.0242\n",
      "Epoch [157/200], Loss: 0.0237\n",
      "Epoch [158/200], Loss: 0.0237\n",
      "Epoch [159/200], Loss: 0.0239\n",
      "Epoch [160/200], Loss: 0.0232\n",
      "Epoch [161/200], Loss: 0.0234\n",
      "Epoch [162/200], Loss: 0.0237\n",
      "Epoch [163/200], Loss: 0.0229\n",
      "Epoch [164/200], Loss: 0.0232\n",
      "Epoch [165/200], Loss: 0.0229\n",
      "Epoch [166/200], Loss: 0.0230\n",
      "Epoch [167/200], Loss: 0.0227\n",
      "Epoch [168/200], Loss: 0.0225\n",
      "Epoch [169/200], Loss: 0.0225\n",
      "Epoch [170/200], Loss: 0.0228\n",
      "Epoch [171/200], Loss: 0.0223\n",
      "Epoch [172/200], Loss: 0.0225\n",
      "Epoch [173/200], Loss: 0.0223\n",
      "Epoch [174/200], Loss: 0.0222\n",
      "Epoch [175/200], Loss: 0.0218\n",
      "Epoch [176/200], Loss: 0.0223\n",
      "Epoch [177/200], Loss: 0.0217\n",
      "Epoch [178/200], Loss: 0.0218\n",
      "Epoch [179/200], Loss: 0.0217\n",
      "Epoch [180/200], Loss: 0.0218\n",
      "Epoch [181/200], Loss: 0.0215\n",
      "Epoch [182/200], Loss: 0.0213\n",
      "Epoch [183/200], Loss: 0.0214\n",
      "Epoch [184/200], Loss: 0.0215\n",
      "Epoch [185/200], Loss: 0.0211\n",
      "Epoch [186/200], Loss: 0.0211\n",
      "Epoch [187/200], Loss: 0.0209\n",
      "Epoch [188/200], Loss: 0.0208\n",
      "Epoch [189/200], Loss: 0.0210\n",
      "Epoch [190/200], Loss: 0.0206\n",
      "Epoch [191/200], Loss: 0.0208\n",
      "Epoch [192/200], Loss: 0.0205\n",
      "Epoch [193/200], Loss: 0.0206\n",
      "Epoch [194/200], Loss: 0.0204\n",
      "Epoch [195/200], Loss: 0.0205\n",
      "Epoch [196/200], Loss: 0.0199\n",
      "Epoch [197/200], Loss: 0.0206\n",
      "Epoch [198/200], Loss: 0.0196\n",
      "Epoch [199/200], Loss: 0.0202\n",
      "Epoch [200/200], Loss: 0.0196\n",
      "=== training k=16 model complete===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if testing_mode:\n",
    "\tprint('Tested')\n",
    "# check if folder exists\n",
    "# elif os.path.isdir('models'):\n",
    "# \tmodel = torch.load('amazon_model.pt')\n",
    "# \tmodel.load_state_dict(torch.load('amazon_model_weights.pt'))\n",
    "else:\n",
    "    batch_size = 128\n",
    "    for bottleneck in bottlenecks:\n",
    "        print(f\"=== training k={bottleneck} model ===\\n\")\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('models/weights', exist_ok=True)\n",
    "        os.makedirs('models/model', exist_ok=True)\n",
    "        fit_by_bottleneck(user_ratings, device, bottleneck)\n",
    "        print(f\"=== training k={bottleneck} model complete===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sample_model(model: AutoEncoder, interaction_matrix: pd.DataFrame, device: torch.device, sample_size: int = 10):\n",
    "# \tmodel.eval()\n",
    "\t\n",
    "# \tsampled = interaction_matrix.sample(sample_size)\n",
    "# \tdisplay(sampled)\n",
    "# \ttested = sampled.apply(lambda row: mask_test_model(model, 0.2, torch.tensor(row.to_numpy(), dtype=torch.float32, device=device)[0], device), axis=0).to_numpy()\n",
    "# \t# s = torch.tensor(sampled)\n",
    "# \treturn linalg.norm(sampled.to_numpy() - tested, ord='fro'), tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(df, user_col='User_id', book_col='book_id'):\n",
    "    \"\"\"\n",
    "    Generate mappings from user and book labels to integer indices.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing user and book columns.\n",
    "    - user_col (str): Name of the user column in the DataFrame.\n",
    "    - book_col (str): Name of the book column in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    - user_mapping (dict): Mapping from user labels to integer indices.\n",
    "    - book_mapping (dict): Mapping from book labels to integer indices.\n",
    "    \"\"\"\n",
    "    user_mapping = {user_id: idx for idx, user_id in enumerate(df[user_col].unique())}\n",
    "    book_mapping = {book_id: idx for idx, book_id in enumerate(df[book_col].unique())}\n",
    "    \n",
    "    return user_mapping, book_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_from_coo(coo, user_label, book_label, user_mapping, book_mapping):\n",
    "    \"\"\"\n",
    "    Get the value from a COO matrix using user and book labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - coo (coo_matrix): The COO matrix.\n",
    "    - user_label (str): The user label.\n",
    "    - book_label (str): The book label.\n",
    "    - user_mapping (dict): Mapping from user labels to integer indices.\n",
    "    - book_mapping (dict): Mapping from book labels to integer indices.\n",
    "    \n",
    "    Returns:\n",
    "    - value (float or None): The value from the COO matrix, or None if not found.\n",
    "    \"\"\"\n",
    "    user_idx = user_mapping.get(user_label)\n",
    "    book_idx = book_mapping.get(book_label)\n",
    "    if user_idx is not None and book_idx is not None:\n",
    "        # Convert to CSR format to make element access efficient\n",
    "        coo_csr = coo.tocsr()\n",
    "        return coo_csr[user_idx, book_idx]\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_sample(sample_coo, mask_fraction: float, device: torch.device):\n",
    "    tensor = torch.tensor(sample_coo.toarray(), dtype=torch.float32, device=device)\n",
    "    tensor = tensor.to(device)\n",
    "    masked_tensor = tensor.clone()\n",
    "\n",
    "    # Identify non-zero elements in the tensor\n",
    "    non_zero_indices = (tensor != 0).nonzero(as_tuple=False)\n",
    "\n",
    "    # Determine the number of elements to mask\n",
    "    num_non_zeros = non_zero_indices.size(0)\n",
    "    num_to_mask = int(mask_fraction * num_non_zeros)\n",
    "\n",
    "    if num_to_mask > 0:\n",
    "        # Randomly select indices to mask\n",
    "        mask_indices = torch.randperm(num_non_zeros)[:num_to_mask]\n",
    "\n",
    "        # Apply the mask\n",
    "        masked_tensor[non_zero_indices[mask_indices, 0], non_zero_indices[mask_indices, 1]] = 0\n",
    "\n",
    "    return masked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_model(model: nn.Module, masked_sample, user_ratings: pd.DataFrame, device: torch.device):\n",
    "    model.eval()\n",
    "    sample_coo = convert_df_coo(user_ratings)\n",
    "    \n",
    "    # Convert the COO matrix to a dense tensor\n",
    "    sample_tensor = torch.tensor(sample_coo.toarray(), dtype=torch.float32, device=device)\n",
    "    \n",
    "    \n",
    "    # Get the model's predictions\n",
    "    with torch.no_grad():\n",
    "        result_tensor = model(masked_sample).cpu().detach().numpy()\n",
    "    \n",
    "    # Extract the non-zero elements from the result tensor\n",
    "    result_values = result_tensor[sample_coo.row, sample_coo.col]\n",
    "    \n",
    "    # Create the result COO matrix using the same row and column indices\n",
    "    result_coo = coo_matrix((result_values, (sample_coo.row, sample_coo.col)), shape=sample_coo.shape)\n",
    "    \n",
    "    # Calculate the RMSE\n",
    "    mse = np.mean((sample_tensor.cpu().numpy() - result_tensor) ** 2)\n",
    "    loss = np.sqrt(mse)\n",
    "    \n",
    "    return loss, result_coo, sample_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bottleneck(user_ratings: pd.DataFrame, masked_sample, device: torch.device, bottleneck: int = 10):\n",
    "    print(f'Testing bottleneck {bottleneck}')\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.load(f'models/model/model_k={bottleneck}.pt')\n",
    "        model.load_state_dict(torch.load(f'models/weights/smodel_k_weights_{bottleneck}.pt'))\n",
    "    else:\n",
    "        model = torch.load(f'models/model/model_k={bottleneck}.pt', map_location=torch.device('cpu') )\n",
    "        model.load_state_dict(torch.load(f'models/weights/smodel_k_weights_{bottleneck}.pt', map_location=torch.device('cpu')))\n",
    "    return test_sample_model(model, masked_sample, user_ratings, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coo = convert_df_coo(user_ratings)\n",
    "# dataset = SparseDataset(coo)\n",
    "# model = AutoEncoder(coo.shape[1], 10, device=device).to(device)\n",
    "# batch_size = 128\n",
    "\n",
    "# batches = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# epochs, losses = model.fit(batches, n_epochs=1000, lr=0.001, patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_loss(bottleneck: int):\n",
    "\tloss = pd.read_csv(f'models/training_losses/{bottleneck}.csv')\n",
    "\tplt.plot(loss['epochs'], loss['training losses'])\n",
    "\tplt.title(f'Training Loss for Bottleneck Size {bottleneck}')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each bottleneck, we found the training loss as show below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for bottleneck in bottlenecks:\n",
    "\t\n",
    "\tplot_training_loss(bottleneck)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tested the performances on each bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_coo = convert_df_coo(user_ratings)\n",
    "masked_sample = mask_sample(sample_coo, 0.05, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the masking technique on the rows for each bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bottleneck 16\n"
     ]
    }
   ],
   "source": [
    "bottleneck_results = [test_bottleneck(user_ratings, masked_sample, device, k) for k in bottlenecks]\n",
    "# bottleneck_results = test_bottleneck(user_ratings, device, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counted how many rows that match the original one. The resulting `coo_matrix` shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapping, book_mapping = get_mappings(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 81.0564\n",
      "Predicted for user 'A30TK6U7DNS82R' and book '0826414346': 272.91485595703125\n"
     ]
    }
   ],
   "source": [
    "# performance_results = pd.DataFrame(columns=['RMSE', 'Non-Zero Count'])\n",
    "for result in bottleneck_results:\n",
    "    print(f'RMSE: {result[0]:.4f}')\n",
    "    user_label = 'A30TK6U7DNS82R'\n",
    "    book_label = '0826414346'\n",
    "    value = get_value_from_coo(result[1], user_label, book_label, user_mapping, book_mapping)\n",
    "    print(f\"Predicted for user '{user_label}' and book '{book_label}': {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for user 'A30TK6U7DNS82R' and book '0826414346': 5.0\n"
     ]
    }
   ],
   "source": [
    "user_label = 'A30TK6U7DNS82R'\n",
    "book_label = '0826414346'\n",
    "value = get_value_from_coo(sample_coo, user_label, book_label, user_mapping, book_mapping)\n",
    "print(f\"Value for user '{user_label}' and book '{book_label}': {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the RMSE, the trainig loss and inference in the sample reveals bottleneck size of 16 is the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if not test16:\n",
    "    b16result = bottleneck_results[1]\n",
    "    pickle.dump(b16result, open('k=16.pkl', 'wb'))\n",
    "else:\n",
    "    file = open('k=16.pkl', 'rb')\n",
    "    b16result = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10207582,\n",
       " <6842x92327 sparse matrix of type '<class 'numpy.float32'>'\n",
       " \twith 406032 stored elements in COOrdinate format>,\n",
       " <6842x92327 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 406032 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b16result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0826414346</th>\n",
       "      <th>0829814000</th>\n",
       "      <th>0963923080</th>\n",
       "      <th>0854968350</th>\n",
       "      <th>0792391810</th>\n",
       "      <th>0789480662</th>\n",
       "      <th>B000857LFE</th>\n",
       "      <th>1884734766</th>\n",
       "      <th>157067051X</th>\n",
       "      <th>B0007DVHU2</th>\n",
       "      <th>...</th>\n",
       "      <th>0762417137</th>\n",
       "      <th>0970045867</th>\n",
       "      <th>1560540990</th>\n",
       "      <th>1855854929</th>\n",
       "      <th>0425200736</th>\n",
       "      <th>B000O00U1K</th>\n",
       "      <th>0786182431</th>\n",
       "      <th>B00085PL4C</th>\n",
       "      <th>0255364520</th>\n",
       "      <th>B000NSLVCU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A30TK6U7DNS82R</th>\n",
       "      <td>2.815453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2MVUWT453QH61</th>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2F6NONFUDB6UK</th>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A14OJS0VWMOSWO</th>\n",
       "      <td>3.121191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.007054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.064209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A373VVEU6Z9M0N</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A36DPPC53ANNG7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1FAWFMP6CDKG</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATBHMBQEC4063</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1UJ3PQFCT0HRH</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3FI8Z8NEFOTHW</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6842 rows × 92327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0826414346  0829814000  0963923080  0854968350  0792391810  \\\n",
       "A30TK6U7DNS82R    2.815453    0.000000         0.0         0.0         0.0   \n",
       "A2MVUWT453QH61    0.000778    0.000000         0.0         0.0         0.0   \n",
       "A2F6NONFUDB6UK    0.001292    0.000000         0.0         0.0         0.0   \n",
       "A14OJS0VWMOSWO    3.121191    0.000000         0.0         0.0         0.0   \n",
       "A373VVEU6Z9M0N    0.000000    0.002893         0.0         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "A36DPPC53ANNG7    0.000000    0.000000         0.0         0.0         0.0   \n",
       "A1FAWFMP6CDKG     0.000000    0.000000         0.0         0.0         0.0   \n",
       "ATBHMBQEC4063     0.000000    0.000000         0.0         0.0         0.0   \n",
       "A1UJ3PQFCT0HRH    0.000000    0.000000         0.0         0.0         0.0   \n",
       "A3FI8Z8NEFOTHW    0.000000    0.000000         0.0         0.0         0.0   \n",
       "\n",
       "                0789480662  B000857LFE  1884734766  157067051X  B0007DVHU2  \\\n",
       "A30TK6U7DNS82R         0.0         0.0    0.000000         0.0         0.0   \n",
       "A2MVUWT453QH61         0.0         0.0    0.000000         0.0         0.0   \n",
       "A2F6NONFUDB6UK         0.0         0.0    0.000000         0.0         0.0   \n",
       "A14OJS0VWMOSWO         0.0         0.0    3.007054         0.0         0.0   \n",
       "A373VVEU6Z9M0N         0.0         0.0    0.000000         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "A36DPPC53ANNG7         0.0         0.0    0.000000         0.0         0.0   \n",
       "A1FAWFMP6CDKG          0.0         0.0    0.000000         0.0         0.0   \n",
       "ATBHMBQEC4063          0.0         0.0    0.000000         0.0         0.0   \n",
       "A1UJ3PQFCT0HRH         0.0         0.0    0.000000         0.0         0.0   \n",
       "A3FI8Z8NEFOTHW         0.0         0.0    0.000000         0.0         0.0   \n",
       "\n",
       "                ...  0762417137  0970045867  1560540990  1855854929  \\\n",
       "A30TK6U7DNS82R  ...    0.000000         0.0         0.0         0.0   \n",
       "A2MVUWT453QH61  ...    0.000000         0.0         0.0         0.0   \n",
       "A2F6NONFUDB6UK  ...    0.000000         0.0         0.0         0.0   \n",
       "A14OJS0VWMOSWO  ...    3.064209         0.0         0.0         0.0   \n",
       "A373VVEU6Z9M0N  ...    0.000000         0.0         0.0         0.0   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "A36DPPC53ANNG7  ...    0.000000         0.0         0.0         0.0   \n",
       "A1FAWFMP6CDKG   ...    0.000000         0.0         0.0         0.0   \n",
       "ATBHMBQEC4063   ...    0.000000         0.0         0.0         0.0   \n",
       "A1UJ3PQFCT0HRH  ...    0.000000         0.0         0.0         0.0   \n",
       "A3FI8Z8NEFOTHW  ...    0.000000         0.0         0.0         0.0   \n",
       "\n",
       "                0425200736  B000O00U1K  0786182431  B00085PL4C  0255364520  \\\n",
       "A30TK6U7DNS82R         0.0         0.0         0.0         0.0         0.0   \n",
       "A2MVUWT453QH61         0.0         0.0         0.0         0.0         0.0   \n",
       "A2F6NONFUDB6UK         0.0         0.0         0.0         0.0         0.0   \n",
       "A14OJS0VWMOSWO         0.0         0.0         0.0         0.0         0.0   \n",
       "A373VVEU6Z9M0N         0.0         0.0         0.0         0.0         0.0   \n",
       "...                    ...         ...         ...         ...         ...   \n",
       "A36DPPC53ANNG7         0.0         0.0         0.0         0.0         0.0   \n",
       "A1FAWFMP6CDKG          0.0         0.0         0.0         0.0         0.0   \n",
       "ATBHMBQEC4063          0.0         0.0         0.0         0.0         0.0   \n",
       "A1UJ3PQFCT0HRH         0.0         0.0         0.0         0.0         0.0   \n",
       "A3FI8Z8NEFOTHW         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "                B000NSLVCU  \n",
       "A30TK6U7DNS82R         0.0  \n",
       "A2MVUWT453QH61         0.0  \n",
       "A2F6NONFUDB6UK         0.0  \n",
       "A14OJS0VWMOSWO         0.0  \n",
       "A373VVEU6Z9M0N         0.0  \n",
       "...                    ...  \n",
       "A36DPPC53ANNG7         0.0  \n",
       "A1FAWFMP6CDKG          0.0  \n",
       "ATBHMBQEC4063          0.0  \n",
       "A1UJ3PQFCT0HRH         0.0  \n",
       "A3FI8Z8NEFOTHW         0.0  \n",
       "\n",
       "[6842 rows x 92327 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_matrix = b16result[1].toarray()\n",
    "dense_df = pd.DataFrame(dense_matrix, index=user_mapping.keys(), columns=book_mapping.keys())\n",
    "dense_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for user 'A14OJS0VWMOSWO' and book '1884734766': 3.5152225494384766\n",
      "Value for user 'A14OJS0VWMOSWO' and book '1884734766': 5.0\n"
     ]
    }
   ],
   "source": [
    "# dense_df.loc[][\"0743491912\"]\n",
    "user_label = \"A14OJS0VWMOSWO\"\n",
    "book_label = \"1884734766\"\n",
    "value = get_value_from_coo(bottleneck_results[1][1], user_label, book_label, user_mapping, book_mapping)\n",
    "print(f\"Value for user '{user_label}' and book '{book_label}': {value}\")\n",
    "value = get_value_from_coo(sample_coo, user_label, book_label, user_mapping, book_mapping)\n",
    "print(f\"Value for user '{user_label}' and book '{book_label}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for user 'A1Z5F7SZDCI5KB' and book 'B000MCASDU': 0.03236457705497742\n",
      "Value for user 'A1Z5F7SZDCI5KB' and book 'B000MCASDU': 2.0\n"
     ]
    }
   ],
   "source": [
    "user_label= \"A1Z5F7SZDCI5KB\"\n",
    "book_label=\"B000MCASDU\"\n",
    "value = get_value_from_coo(bottleneck_results[2][1], user_label, book_label, user_mapping, book_mapping)\n",
    "print(f\"Value for user '{user_label}' and book '{book_label}': {value}\")\n",
    "value = get_value_from_coo(sample_coo, user_label, book_label, user_mapping, book_mapping)\n",
    "print(f\"Value for user '{user_label}' and book '{book_label}': {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data analyis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
