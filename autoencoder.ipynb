{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the autoencoder, we chose Pytorch for its similarity to numpy and compatibility to gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Matrix below was used to debug and test the performance before applying it to the actual matrix.\n",
    "The matrix is a sparse integer matrix of 100 users by 50 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Interaction Matrix (first 10 users):\n",
      "[[3 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 4]\n",
      " [0 0 0 ... 0 0 5]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " [0 0 4 ... 1 0 0]]\n",
      "\n",
      "Noisy Interaction Matrix (first 10 users):\n",
      "[[3.5780747  0.44618523 0.27021518 ... 0.         0.19485128 0.02848756]\n",
      " [0.         4.805831   0.         ... 0.         0.         3.971974  ]\n",
      " [0.         0.12133284 0.6427789  ... 0.         0.         4.9596815 ]\n",
      " ...\n",
      " [0.16250879 0.08124392 0.19579445 ... 0.09819969 0.18892097 0.5638384 ]\n",
      " [0.         0.3908983  1.8214145  ... 0.         0.         0.        ]\n",
      " [0.07031945 0.         3.5793095  ... 0.51570594 0.         0.36294198]]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a user-item interaction matrix (100 users, 50 items) with sparsity\n",
    "num_users = 10000\n",
    "num_items = 500\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_items))  # Random interactions from 0 to 5\n",
    "\n",
    "# Introduce sparsity by setting a high percentage of interactions to 0\n",
    "sparsity = 0.8  # 80% of the interactions will be set to 0\n",
    "mask = np.random.rand(*interaction_matrix.shape) < sparsity\n",
    "interaction_matrix[mask] = 0\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "interaction_tensor = torch.tensor(interaction_matrix, dtype=torch.float32)\n",
    "\n",
    "# Add noise to the input data\n",
    "def add_noise(data, noise_factor=0.3):\n",
    "    noisy_data = data + noise_factor * torch.randn_like(data)\n",
    "    noisy_data = torch.clamp(noisy_data, 0., 5.)  # Ensure values stay within the interaction range\n",
    "    return noisy_data\n",
    "\n",
    "noisy_interaction_tensor = add_noise(interaction_tensor)\n",
    "\n",
    "# Print the original and noisy matrices (first 10 users for brevity)\n",
    "print(\"Original Interaction Matrix (first 10 users):\")\n",
    "print(interaction_matrix[:10])  # Print only the first 10 users for brevity\n",
    "print(\"\\nNoisy Interaction Matrix (first 10 users):\")\n",
    "print(noisy_interaction_tensor[:10].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\tdef __init__(self, input_dim, bottleneck_size, device='cpu'):\n",
    "\t\tsuper(AutoEncoder, self).__init__()\n",
    "\t\tself.device = device\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, bottleneck_size)\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(bottleneck_size, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, input_dim)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\tx = self.decoder(x)\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef fit(self, batches, n_epochs=100, min_delta=0.0001, lr=0.001, patience=10):\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\tbest_loss = float('inf')\n",
    "\t\tpatience_counter = 0\n",
    "\t\tepoch_losses = []\n",
    "\t\tepochs = []\n",
    "\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\tepoch_loss = 0.0\n",
    "\t\t\tfor batch in batches:\n",
    "\t\t\t\tbatch = batch[0].to(self.device)  # Move batch to device\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\toutput = self.forward(batch)\n",
    "\t\t\t\tloss = criterion(output, batch)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\t\t\n",
    "\t\t\tepoch_loss /= len(batches)\n",
    "\n",
    "\t\t\tif epoch_loss < best_loss - min_delta:\n",
    "\t\t\t\tbest_loss = epoch_loss\n",
    "\t\t\t\tpatience_counter = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tpatience_counter += 1\n",
    "\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
    "\t\t\t\tbreak\n",
    "\t\t\tepoch_losses.append(epoch_loss)\n",
    "\t\t\tepochs.append(epoch+1)\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\t\treturn epochs, epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "train = TensorDataset(noisy_interaction_tensor)\n",
    "batches = DataLoader(train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.5313\n",
      "Epoch [2/1000], Loss: 1.4812\n",
      "Epoch [3/1000], Loss: 1.4615\n",
      "Epoch [4/1000], Loss: 1.4445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m bottleneck_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoEncoder(input_dim, bottleneck_size, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[111], line 35\u001b[0m, in \u001b[0;36mAutoEncoder.fit\u001b[0;34m(self, batches, n_epochs, min_delta, lr, patience)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     34\u001b[0m \tepoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43m\t\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Move batch to device\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/torch/utils/data/dataset.py:206\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:364\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m NO_FTRACE\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# if thread is not alive, cancel trace_dispatch processing\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_thread_alive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    365\u001b[0m     py_db\u001b[38;5;241m.\u001b[39mnotify_thread_not_alive(get_current_thread_id(t))\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m NO_FTRACE\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the Autoencoder model\n",
    "input_dim = num_items\n",
    "bottleneck_size = 64\n",
    "model = AutoEncoder(input_dim, bottleneck_size, device=device).to(device)\n",
    "model.fit(batches, n_epochs=1000, lr=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then trained the model using Google Colab VM with nvidia's A100 gpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `testing_mode` is a checkpoint variable to signify that we are testing the model on the local end. If set to false, it will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('full_matrix.csv.gzip', compression='gzip', index_col=0)\n",
    "user_ratings.notna().count().sum()\n",
    "user_ratings = user_ratings.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A101446I5AWY0Z</th>\n",
       "      <th>A103U0Q3IKSXHE</th>\n",
       "      <th>A103W7ZPKGOCC9</th>\n",
       "      <th>A105E427BB6J65</th>\n",
       "      <th>A106016KSI0YQ</th>\n",
       "      <th>A106E1N0ZQ4D9W</th>\n",
       "      <th>A1075MZNVRMSEO</th>\n",
       "      <th>A107C4RVRF0OP</th>\n",
       "      <th>A107YFBJ119GZR</th>\n",
       "      <th>A10872FHIJAKKD</th>\n",
       "      <th>...</th>\n",
       "      <th>AZVZSGHKV0AO0</th>\n",
       "      <th>AZWC9XAY34IPW</th>\n",
       "      <th>AZWG3PF80735Q</th>\n",
       "      <th>AZWOQXRCS1WA6</th>\n",
       "      <th>AZWW1U604W0N</th>\n",
       "      <th>AZXEZRXZQL1H2</th>\n",
       "      <th>AZXGPM8EKSHE9</th>\n",
       "      <th>AZXQKAMHK35PA</th>\n",
       "      <th>AZY8LGHVF8GMZ</th>\n",
       "      <th>AZZVZL4QEHEHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1882931173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0826414346</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0829814000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0595344550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0253338352</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0590482467</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0570047870</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B000OVF7JY</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402508735</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0534400604</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206711 rows × 6842 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A101446I5AWY0Z  A103U0Q3IKSXHE  A103W7ZPKGOCC9  A105E427BB6J65  \\\n",
       "1882931173             0.0             0.0             0.0             0.0   \n",
       "0826414346             0.0             0.0             0.0             0.0   \n",
       "0829814000             0.0             0.0             0.0             0.0   \n",
       "0595344550             0.0             0.0             0.0             0.0   \n",
       "0253338352             0.0             0.0             0.0             0.0   \n",
       "...                    ...             ...             ...             ...   \n",
       "0590482467             0.0             0.0             0.0             0.0   \n",
       "0570047870             0.0             0.0             0.0             0.0   \n",
       "B000OVF7JY             0.0             0.0             0.0             0.0   \n",
       "1402508735             0.0             0.0             0.0             0.0   \n",
       "0534400604             0.0             0.0             0.0             0.0   \n",
       "\n",
       "            A106016KSI0YQ  A106E1N0ZQ4D9W  A1075MZNVRMSEO  A107C4RVRF0OP  \\\n",
       "1882931173            0.0             0.0             0.0            0.0   \n",
       "0826414346            0.0             0.0             0.0            0.0   \n",
       "0829814000            0.0             0.0             0.0            0.0   \n",
       "0595344550            0.0             0.0             0.0            0.0   \n",
       "0253338352            0.0             0.0             0.0            0.0   \n",
       "...                   ...             ...             ...            ...   \n",
       "0590482467            0.0             0.0             0.0            0.0   \n",
       "0570047870            0.0             0.0             0.0            0.0   \n",
       "B000OVF7JY            0.0             0.0             0.0            0.0   \n",
       "1402508735            0.0             0.0             0.0            0.0   \n",
       "0534400604            0.0             0.0             0.0            0.0   \n",
       "\n",
       "            A107YFBJ119GZR  A10872FHIJAKKD  ...  AZVZSGHKV0AO0  AZWC9XAY34IPW  \\\n",
       "1882931173             0.0             0.0  ...            0.0            0.0   \n",
       "0826414346             0.0             0.0  ...            0.0            0.0   \n",
       "0829814000             0.0             0.0  ...            0.0            0.0   \n",
       "0595344550             0.0             0.0  ...            0.0            0.0   \n",
       "0253338352             0.0             0.0  ...            0.0            0.0   \n",
       "...                    ...             ...  ...            ...            ...   \n",
       "0590482467             0.0             0.0  ...            0.0            0.0   \n",
       "0570047870             0.0             0.0  ...            0.0            0.0   \n",
       "B000OVF7JY             0.0             0.0  ...            0.0            0.0   \n",
       "1402508735             0.0             0.0  ...            0.0            0.0   \n",
       "0534400604             0.0             0.0  ...            0.0            0.0   \n",
       "\n",
       "            AZWG3PF80735Q  AZWOQXRCS1WA6  AZWW1U604W0N  AZXEZRXZQL1H2  \\\n",
       "1882931173            0.0            0.0           0.0            0.0   \n",
       "0826414346            0.0            0.0           0.0            0.0   \n",
       "0829814000            0.0            0.0           0.0            0.0   \n",
       "0595344550            0.0            0.0           0.0            0.0   \n",
       "0253338352            0.0            0.0           0.0            0.0   \n",
       "...                   ...            ...           ...            ...   \n",
       "0590482467            0.0            0.0           0.0            0.0   \n",
       "0570047870            0.0            0.0           0.0            0.0   \n",
       "B000OVF7JY            0.0            0.0           0.0            0.0   \n",
       "1402508735            0.0            0.0           0.0            0.0   \n",
       "0534400604            0.0            0.0           0.0            0.0   \n",
       "\n",
       "            AZXGPM8EKSHE9  AZXQKAMHK35PA  AZY8LGHVF8GMZ  AZZVZL4QEHEHO  \n",
       "1882931173            0.0            0.0            0.0            0.0  \n",
       "0826414346            0.0            0.0            0.0            0.0  \n",
       "0829814000            0.0            0.0            0.0            0.0  \n",
       "0595344550            0.0            0.0            0.0            0.0  \n",
       "0253338352            0.0            0.0            0.0            0.0  \n",
       "...                   ...            ...            ...            ...  \n",
       "0590482467            0.0            0.0            0.0            0.0  \n",
       "0570047870            0.0            0.0            0.0            0.0  \n",
       "B000OVF7JY            0.0            0.0            0.0            0.0  \n",
       "1402508735            0.0            0.0            0.0            0.0  \n",
       "0534400604            0.0            0.0            0.0            0.0  \n",
       "\n",
       "[206711 rows x 6842 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training on colab, we utilized the pytorch imports the weights and the model architecture into the \n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is a significant difference between bottlenecks, we decided to run it with different bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottlenecks = [8, 16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we used the batch size of 128 and parameters of 1000 epoch, learning rate of 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if testing_mode:\n",
    "\tprint('Tested')\n",
    "# check if folder exists\n",
    "elif os.path.isdir('models'):\n",
    "\tmodel = torch.load('amazon_model.pt')\n",
    "\tmodel.load_state_dict(torch.load('amazon_model_weights.pt'))\n",
    "else:\n",
    "\ttensors = torch.tensor(user_ratings.to_numpy(), dtype=torch.float32, device=device)\n",
    "\tbatch_size = 128\n",
    "\tfor bottleneck in bottlenecks:\n",
    "\t\tmodel = AutoEncoder(input_dim, bottleneck, device=device).to(device)\n",
    "\t\tbatches = DataLoader(TensorDataset(tensors), batch_size=batch_size, shuffle=True)\n",
    "\t\tepochs, losses = model.fit(batches, n_epochs=1000, lr=0.001, patience=10)\n",
    "\t\tos.makedirs('models', exist_ok=True)\n",
    "\t\tos.makedirs('models/weights', exist_ok=True)\n",
    "\t\tos.makedirs('models/model', exist_ok=True)\n",
    "\t\ttorch.save(model, f'models/model/model_k={bottleneck}.pt')\n",
    "\t\ttorch.save(model.state_dict(), f'models/weights/model_weights_{bottleneck}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not testing_mode:\n",
    "\trow = torch.tensor(user_ratings.iloc[0].to_numpy(), dtype=torch.float32, device=device)\n",
    "\tres , _ = mask_test_model(model, 0.2, row, user_ratings, device)\n",
    "\tres[res >= 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_sample_model(model: AutoEncoder, interaction_matrix: pd.DataFrame, device: torch.device, sample_size: int = 10):\n",
    "# \tmodel.eval()\n",
    "\t\n",
    "# \tsampled = interaction_matrix.sample(sample_size)\n",
    "# \tdisplay(sampled)\n",
    "# \ttested = sampled.apply(lambda row: mask_test_model(model, 0.2, torch.tensor(row.to_numpy(), dtype=torch.float32, device=device)[0], device), axis=0).to_numpy()\n",
    "# \t# s = torch.tensor(sampled)\n",
    "# \treturn linalg.norm(sampled.to_numpy() - tested, ord='fro'), tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_by_bottleneck(user_ratings: pd.DataFrame, device: torch.device, bottleneck: int = 10):\n",
    "\ttensors = torch.tensor(user_ratings.to_numpy(), dtype=torch.float32, device=device)\n",
    "\tbatch_size = 32 \n",
    "\ttrain = TensorDataset(tensors)\n",
    "\tbatches = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\tmodel = AutoEncoder(50, bottleneck)\n",
    "\tres = model.fit(batches)\n",
    "\tpd.DataFrame({\"epochs\": res[0], \"losses\": res[1]}).to_csv(f'losses_k={bottleneck}.csv')\n",
    "\t# save the model\n",
    "\ttorch.save(model, f'model_k={bottleneck}.pt')\n",
    "\ttorch.save(model.state_dict(), f'model_weights_{bottleneck}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not testing_mode:\n",
    "\tfit_by_bottleneck(user_ratings, device, 5)\n",
    "\tfit_by_bottleneck(user_ratings, device, 10)\n",
    "\tfit_by_bottleneck(user_ratings, device, 15)\n",
    "\tfit_by_bottleneck(user_ratings, device, 20)\n",
    "\tfit_by_bottleneck(user_ratings, device, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the performance for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_test_model(model: nn.Module, mask_fraction: float, row: torch.Tensor, device: torch.device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Identify non-zero elements in the row\n",
    "        non_zero_indices = row.nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Create a mask for the non-zero elements\n",
    "        mask = torch.rand(len(non_zero_indices)).to(device) < mask_fraction\n",
    "\n",
    "        # Apply the mask to the row\n",
    "        masked_row = row.clone().to(device)\n",
    "        masked_row[non_zero_indices[mask]] = 0\n",
    "\n",
    "        # Get the model's predictions\n",
    "        predictions = model(masked_row)\n",
    "\n",
    "        # Calculate the loss only for the masked values\n",
    "        loss = criterion(predictions[non_zero_indices[mask]], row[non_zero_indices[mask]])\n",
    "\n",
    "        # Optionally, return the predictions and mask for further analysis\n",
    "        return predictions.cpu().numpy(), mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_model(model: AutoEncoder, sample:pd.DataFrame, device: torch.device):\n",
    "    model.eval()\n",
    "    # sample = interaction_matrix.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Convert the sample DataFrame to a tensor\n",
    "    sample_tensor = torch.tensor(sample.to_numpy(), dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Test the model on each row of the sample\n",
    "    # tested = sample.apply(lambda row: mask_test_model(model, 0.2, torch.tensor(row.to_numpy(), dtype=torch.float32, device=device), device)[0], axis=1)\n",
    "    tested = []\n",
    "    for row in sample.iterrows():\n",
    "        prediction = mask_test_model(model, 0.2, torch.tensor(row[1].to_numpy(), dtype=torch.float32, device=device), device)[0]\n",
    "        # display(prediction)\n",
    "        prediction = pd.DataFrame(prediction)\n",
    "        tested.append(prediction)\n",
    "    \n",
    "\n",
    "    tested = pd.concat(tested, axis=1)\n",
    "    tested.index = sample.columns\n",
    "    tested.columns = sample.index\n",
    "\n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    # calculate the rmse\n",
    "    \n",
    "    # loss = linalg.norm(sample.to_numpy() - tested.to_numpy().T, ord='fro')\n",
    "    mse = np.mean((sample.to_numpy() - tested.to_numpy().T)**2)\n",
    "    loss = np.sqrt(mse)\n",
    "    \n",
    "    return loss, tested.T, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bottleneck(user_ratings: pd.DataFrame, device: torch.device, sample, bottleneck: int = 10):\n",
    "    print(f'Testing bottleneck {bottleneck}')\n",
    "    model = torch.load(f'models/model/model_k={bottleneck}.pt')\n",
    "    model.load_state_dict(torch.load(f'models/weights/model_weights_{bottleneck}.pt'))\n",
    "    return test_sample_model(model, user_ratings, device, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_traing_loss(bottleneck: int):\n",
    "\tloss = pd.read_csv(f'losses_k={bottleneck}.csv')\n",
    "\tplt.plot(loss['epochs'], loss['losses'])\n",
    "\tplt.title(f'Training Loss for Bottleneck Size {bottleneck}')\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each bottleneck, we found the training loss as show below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bottleneck in bottlenecks:\n",
    "\tfig, ax = plt.subplots()\n",
    "\tplot_traing_loss(bottleneck)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a sample of 100 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = user_ratings.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the masking technique on the rows for each bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing bottleneck 5\n",
      "Testing bottleneck 10\n",
      "Testing bottleneck 15\n",
      "Testing bottleneck 20\n",
      "Testing bottleneck 25\n"
     ]
    }
   ],
   "source": [
    "bottleneck_results = [test_bottleneck(user_ratings, device, k, 100) for k in bottlenecks]\n",
    "# bottleneck_results = test_bottleneck(user_ratings, device, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counted how many rows that match the original one. The resulting dataframe shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_results = pd.DataFrame(columns=['RMSE', 'Masked Value Count'])\n",
    "for result in bottleneck_results:\n",
    "\tprint(f'RMSE: {result[0]:.4f}')\n",
    "\t# display((result[1] > 0.1).sum().sum())\n",
    "\tperformance_results = pd.concat([performance_results, pd.DataFrame({'RMSE': [result[0]], 'Masked Value Count': [(result[1] > 0.1).sum().sum()]})])\n",
    "\n",
    "display(performance_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, notice that the data is very sparse, with \"write the number of n\". This sparseness requires the need for an appropriate modification, namely the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoEncoder(nn.Module):\n",
    "\tdef __init__(self, input_dim, bottleneck_size, device='cpu'):\n",
    "\t\tsuper(SparseAutoEncoder, self).__init__()\n",
    "\t\tself.device = device\n",
    "\t\tself.encoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(input_dim, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, bottleneck_size)\n",
    "\t\t)\n",
    "\t\tself.decoder = nn.Sequential(\n",
    "\t\t\tnn.Linear(bottleneck_size, 64),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(64, 128),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(128, input_dim)\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder(x)\n",
    "\t\ty = self.decoder(x)\n",
    "\t\treturn x, y\n",
    "\t\n",
    "\tdef kl_divergence(self, p, q):\n",
    "\t\t# Compute the KL divergence\n",
    "\t\tp = torch.clamp(torch.tensor(p), 1e-10, 1- 1e-10)  # Avoid log(0)\n",
    "\t\tq = torch.clamp(q, 1e-10, 1-1e-10)  # Avoid log(0)\n",
    "\t\tkl_loss = p * torch.log(p / q) + (1 - p) * torch.log((1 - p) / (1 - q))\n",
    "\t\treturn kl_loss\n",
    "\t\n",
    "\tdef sparse_loss(self, x, sparsity_ratio=0.05, sparsity_weight=0.2):\n",
    "\t\t# Compute the sparsity loss\n",
    "\t\tx = torch.sigmoid(x)\n",
    "\t\tsparsity_loss = self.kl_divergence(sparsity_ratio, torch.mean(x, dim=0))\n",
    "\t\tsparsity_loss = torch.sum(sparsity_loss)\n",
    "\t\treturn sparsity_weight * sparsity_loss\n",
    "\n",
    "\tdef fit(self, batches, n_epochs=100, min_delta=0.0001, lr=0.001, patience=10):\n",
    "\t\toptimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\t\tcriterion = nn.MSELoss()\n",
    "\t\tbest_loss = float('inf')\n",
    "\t\tpatience_counter = 0\n",
    "\n",
    "\t\tfor epoch in range(n_epochs):\n",
    "\t\t\tepoch_loss = 0.0\n",
    "\t\t\tfor batch in batches:\n",
    "\t\t\t\tbatch = batch[0].to(self.device)  # Move batch to device\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tencoded, decoded = self.forward(batch)\n",
    "\t\t\t\tloss = criterion(decoded, batch)\n",
    "\t\t\t\tsparsity_loss = self.sparse_loss(encoded)\n",
    "\t\t\t\tloss += sparsity_loss\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tepoch_loss += loss.item()\n",
    "\n",
    "\t\t\tepoch_loss /= len(batches)\n",
    "\n",
    "\t\t\tif epoch_loss < best_loss - min_delta:\n",
    "\t\t\t\tbest_loss = epoch_loss\n",
    "\t\t\t\tpatience_counter = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tpatience_counter += 1\n",
    "\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(f\"Early stopping at epoch {epoch+1} with loss {epoch_loss:.4f}\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\tprint(f'Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\t\treturn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we test the results using the same hyperparameter and hypothetical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a user-item interaction matrix (100 users, 50 items) with sparsity\n",
    "num_users = 10000\n",
    "num_items = 500\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_items))  # Random interactions from 0 to 5\n",
    "\n",
    "# Introduce sparsity by setting a high percentage of interactions to 0\n",
    "sparsity = 0.8  # 80% of the interactions will be set to 0\n",
    "mask = np.random.rand(*interaction_matrix.shape) < sparsity\n",
    "interaction_matrix[mask] = 0\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "interaction_tensor = torch.tensor(interaction_matrix, dtype=torch.float32)\n",
    "\n",
    "# Add noise to the input data\n",
    "def add_noise(data, noise_factor=0.3):\n",
    "    noisy_data = data + noise_factor * torch.randn_like(data)\n",
    "    noisy_data = torch.clamp(noisy_data, 0., 5.)  # Ensure values stay within the interaction range\n",
    "    return noisy_data\n",
    "\n",
    "noisy_interaction_tensor = add_noise(interaction_tensor)\n",
    "\n",
    "# Print the original and noisy matrices (first 10 users for brevity)\n",
    "print(\"Original Interaction Matrix (first 10 users):\")\n",
    "print(interaction_matrix[:10])  # Print only the first 10 users for brevity\n",
    "print(\"\\nNoisy Interaction Matrix (first 10 users):\")\n",
    "print(noisy_interaction_tensor[:10].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SparseAutoEncoder model\n",
    "input_dim = num_items\n",
    "bottleneck_size = 10\n",
    "model = SparseAutoEncoder(input_dim, bottleneck_size, device=device).to(device)\n",
    "model.fit(batches, n_epochs=1000, lr=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's apply the autoencoder again to our main matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_mode:\n",
    "\tprint('Tested')\n",
    "elif os.path.exists('sparse_model_weights.pt') and os.path.exists('sparse_amazon_model.pt'):\n",
    "\tmodel = torch.load('amazon_model.pt')\n",
    "\tmodel.load_state_dict(torch.load('amazon_model_weights.pt'))\n",
    "\n",
    "else:\n",
    "\ttensors = torch.tensor(user_ratings.to_numpy(), dtype=torch.float32, device=device)\n",
    "\tbatch_size = 128 \n",
    "\ttrain = TensorDataset(tensors)\n",
    "\tbatches = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\tmodel = SparseAutoEncoder(use, 10)\n",
    "\tmodel.fit(batches, 1000, 0.001, 0.0001, 5)\n",
    "\ttorch.save(model, 'sparse_amazon_model.pt')\n",
    "\ttorch.save(model.state_dict(), 'sparse_amazon_model_weights.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "714"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bottleneck_results[1][1] > 0.1).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02031462226076022,\n",
       "             A101446I5AWY0Z  A103U0Q3IKSXHE  A103W7ZPKGOCC9  A105E427BB6J65  \\\n",
       " 0743246500       -0.000202        0.042564        0.004264       -0.002793   \n",
       " 0138421471       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " 0385317999       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " B000C4SS5I       -0.000213       -0.000915        0.007007       -0.000134   \n",
       " B000OTYZHG       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " B0006YV4SW       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " 1885210086       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " 0806504757       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " 059513534X       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " 0155067036       -0.000233       -0.001040        0.006951       -0.000188   \n",
       " \n",
       "             A106016KSI0YQ  A106E1N0ZQ4D9W  A1075MZNVRMSEO  A107C4RVRF0OP  \\\n",
       " 0743246500       0.005651       -0.003634       -0.000546       0.000345   \n",
       " 0138421471       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " 0385317999       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " B000C4SS5I       0.004357       -0.001960       -0.000036      -0.000174   \n",
       " B000OTYZHG       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " B0006YV4SW       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " 1885210086       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " 0806504757       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " 059513534X       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " 0155067036       0.003915       -0.001931       -0.000029      -0.000336   \n",
       " \n",
       "             A107YFBJ119GZR  A10872FHIJAKKD  ...  AZVZSGHKV0AO0  AZWC9XAY34IPW  \\\n",
       " 0743246500        0.003393       -0.002638  ...      -0.006219      -0.001918   \n",
       " 0138421471       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " 0385317999       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " B000C4SS5I       -0.001057       -0.000192  ...      -0.000235      -0.000226   \n",
       " B000OTYZHG       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " B0006YV4SW       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " 1885210086       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " 0806504757       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " 059513534X       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " 0155067036       -0.000967       -0.000537  ...      -0.000481      -0.000209   \n",
       " \n",
       "             AZWG3PF80735Q  AZWOQXRCS1WA6  AZWW1U604W0N  AZXEZRXZQL1H2  \\\n",
       " 0743246500       0.006387      -0.002663      0.017118      -0.001595   \n",
       " 0138421471       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " 0385317999       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " B000C4SS5I       0.008519      -0.000146     -0.000471      -0.000068   \n",
       " B000OTYZHG       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " B0006YV4SW       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " 1885210086       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " 0806504757       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " 059513534X       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " 0155067036       0.008350      -0.000357     -0.000472      -0.000075   \n",
       " \n",
       "             AZXGPM8EKSHE9  AZXQKAMHK35PA  AZY8LGHVF8GMZ  AZZVZL4QEHEHO  \n",
       " 0743246500      -0.003229      -0.005295      -0.005809       0.000104  \n",
       " 0138421471      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " 0385317999      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " B000C4SS5I       0.000001      -0.000639      -0.001531      -0.000064  \n",
       " B000OTYZHG      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " B0006YV4SW      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " 1885210086      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " 0806504757      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " 059513534X      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " 0155067036      -0.000068      -0.000853      -0.001610      -0.000083  \n",
       " \n",
       " [10 rows x 6842 columns],\n",
       "             A101446I5AWY0Z  A103U0Q3IKSXHE  A103W7ZPKGOCC9  A105E427BB6J65  \\\n",
       " 0743246500             0.0             0.0             0.0             0.0   \n",
       " 0138421471             0.0             0.0             0.0             0.0   \n",
       " 0385317999             0.0             0.0             0.0             0.0   \n",
       " B000C4SS5I             0.0             0.0             0.0             0.0   \n",
       " B000OTYZHG             0.0             0.0             0.0             0.0   \n",
       " B0006YV4SW             0.0             0.0             0.0             0.0   \n",
       " 1885210086             0.0             0.0             0.0             0.0   \n",
       " 0806504757             0.0             0.0             0.0             0.0   \n",
       " 059513534X             0.0             0.0             0.0             0.0   \n",
       " 0155067036             0.0             0.0             0.0             0.0   \n",
       " \n",
       "             A106016KSI0YQ  A106E1N0ZQ4D9W  A1075MZNVRMSEO  A107C4RVRF0OP  \\\n",
       " 0743246500            0.0             0.0             0.0            0.0   \n",
       " 0138421471            0.0             0.0             0.0            0.0   \n",
       " 0385317999            0.0             0.0             0.0            0.0   \n",
       " B000C4SS5I            0.0             0.0             0.0            0.0   \n",
       " B000OTYZHG            0.0             0.0             0.0            0.0   \n",
       " B0006YV4SW            0.0             0.0             0.0            0.0   \n",
       " 1885210086            0.0             0.0             0.0            0.0   \n",
       " 0806504757            0.0             0.0             0.0            0.0   \n",
       " 059513534X            0.0             0.0             0.0            0.0   \n",
       " 0155067036            0.0             0.0             0.0            0.0   \n",
       " \n",
       "             A107YFBJ119GZR  A10872FHIJAKKD  ...  AZVZSGHKV0AO0  AZWC9XAY34IPW  \\\n",
       " 0743246500             0.0             0.0  ...            0.0            0.0   \n",
       " 0138421471             0.0             0.0  ...            0.0            0.0   \n",
       " 0385317999             0.0             0.0  ...            0.0            0.0   \n",
       " B000C4SS5I             0.0             0.0  ...            0.0            0.0   \n",
       " B000OTYZHG             0.0             0.0  ...            0.0            0.0   \n",
       " B0006YV4SW             0.0             0.0  ...            0.0            0.0   \n",
       " 1885210086             0.0             0.0  ...            0.0            0.0   \n",
       " 0806504757             0.0             0.0  ...            0.0            0.0   \n",
       " 059513534X             0.0             0.0  ...            0.0            0.0   \n",
       " 0155067036             0.0             0.0  ...            0.0            0.0   \n",
       " \n",
       "             AZWG3PF80735Q  AZWOQXRCS1WA6  AZWW1U604W0N  AZXEZRXZQL1H2  \\\n",
       " 0743246500            0.0            0.0           0.0            0.0   \n",
       " 0138421471            0.0            0.0           0.0            0.0   \n",
       " 0385317999            0.0            0.0           0.0            0.0   \n",
       " B000C4SS5I            0.0            0.0           0.0            0.0   \n",
       " B000OTYZHG            0.0            0.0           0.0            0.0   \n",
       " B0006YV4SW            0.0            0.0           0.0            0.0   \n",
       " 1885210086            0.0            0.0           0.0            0.0   \n",
       " 0806504757            0.0            0.0           0.0            0.0   \n",
       " 059513534X            0.0            0.0           0.0            0.0   \n",
       " 0155067036            0.0            0.0           0.0            0.0   \n",
       " \n",
       "             AZXGPM8EKSHE9  AZXQKAMHK35PA  AZY8LGHVF8GMZ  AZZVZL4QEHEHO  \n",
       " 0743246500            0.0            0.0            0.0            0.0  \n",
       " 0138421471            0.0            0.0            0.0            0.0  \n",
       " 0385317999            0.0            0.0            0.0            0.0  \n",
       " B000C4SS5I            0.0            0.0            0.0            0.0  \n",
       " B000OTYZHG            0.0            0.0            0.0            0.0  \n",
       " B0006YV4SW            0.0            0.0            0.0            0.0  \n",
       " 1885210086            0.0            0.0            0.0            0.0  \n",
       " 0806504757            0.0            0.0            0.0            0.0  \n",
       " 059513534X            0.0            0.0            0.0            0.0  \n",
       " 0155067036            0.0            0.0            0.0            0.0  \n",
       " \n",
       " [10 rows x 6842 columns])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_results[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same methods, we get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bottleneck(user_ratings, device, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data analyis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
